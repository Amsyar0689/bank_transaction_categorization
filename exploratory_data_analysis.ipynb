{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bank_transaction = pd.read_csv(\"bank_transaction.csv\")\n",
    "user_profile = pd.read_csv(\"user_profile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Define a basic list of common English stopwords manually\n",
    "stopwords_list = set([\n",
    "    \"the\", \"is\", \"in\", \"it\", \"of\", \"for\", \"on\", \"with\", \"to\", \"from\", \"by\", \"at\", \n",
    "    \"a\", \"an\", \"and\", \"or\", \"this\", \"that\", \"be\", \"was\", \"were\", \"has\", \"had\", \n",
    "    \"have\", \"as\", \"but\", \"if\", \"then\", \"so\", \"because\", \"about\", \"into\", \"out\", \n",
    "    \"over\", \"under\", \"between\", \"after\", \"before\", \"above\", \"below\", \"again\", \n",
    "    \"once\", \"during\", \"while\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "])\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        text = text.strip()  # Remove extra spaces\n",
    "        text = \" \".join([word for word in text.split() if word not in stopwords_list])  # Remove stopwords\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "bank_transaction[\"cleaned_description\"] = bank_transaction[\"description\"].apply(clean_text)\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(bank_transaction[\"cleaned_description\"])\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "feature_freq = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n",
    "feature_df = pd.DataFrame({'feature': feature_names, 'frequency': feature_freq})\n",
    "threshold = len(bank_transaction) * 0.01  # Adjust as needed\n",
    "selected_features = feature_df[feature_df[\"frequency\"] >= threshold][\"feature\"].tolist()\n",
    "tfidf_filtered = TfidfVectorizer(vocabulary=selected_features)\n",
    "tfidf_matrix_filtered = tfidf_filtered.fit_transform(bank_transaction[\"cleaned_description\"])\n",
    "tfidf_df_filtered = pd.DataFrame(tfidf_matrix_filtered.toarray(), columns=selected_features)\n",
    "\n",
    "processed_df = bank_transaction.drop(columns=[\"description\", \"cleaned_description\"]).join(tfidf_df_filtered)\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert bool to int in user_profile\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "user_profile.columns = user_profile.columns.str.lower()\n",
    "cols = user_profile.select_dtypes(include=['bool']).columns\n",
    "encoder = OneHotEncoder(drop='if_binary', dtype=int)\n",
    "encoded_array = encoder.fit_transform(user_profile[cols]).toarray()\n",
    "new_user_profile = pd.DataFrame(encoded_array, columns=cols)\n",
    "user_profile[cols] = new_user_profile\n",
    "user_profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join both datasets to add coeficients\n",
    "combined = pd.merge(processed_df, user_profile, on='client_id', how='left')\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "combined = combined.dropna(subset=[\"category\"])\n",
    "combined[\"category\"] = combined[\"category\"].astype(\"category\").cat.codes\n",
    "\n",
    "xcols = combined.drop(columns=[\"category\", \"client_id\", \"bank_id\", \"account_id\", \"txn_id\", \"txn_date\"])\n",
    "ycol = combined[\"category\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(xcols, ycol, test_size=0.25, random_state=42, stratify=ycol)\n",
    "\n",
    "model = LogisticRegression(solver=\"saga\", max_iter=1000, tol=1e-3, n_jobs=-1)\n",
    "cv_scores = cross_val_score(model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "print(f\"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "combined = combined.dropna(subset=[\"category\"])\n",
    "combined[\"category\"] = combined[\"category\"].astype(\"category\").cat.codes\n",
    "\n",
    "xcols = combined.drop(columns=[\"category\", \"client_id\", \"bank_id\", \"account_id\", \"txn_id\", \"txn_date\"])\n",
    "ycol = combined[\"category\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(xcols, ycol, test_size=0.25, random_state=42, stratify=ycol)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)  # Using 100 trees, parallel processing\n",
    "cv_scores = cross_val_score(model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "print(f\"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "bank_transaction[\"txn_date\"] = pd.to_datetime(bank_transaction[\"txn_date\"])\n",
    "\n",
    "stopwords_list = set([\n",
    "    \"the\", \"is\", \"in\", \"it\", \"of\", \"for\", \"on\", \"with\", \"to\", \"from\", \"by\", \"at\", \n",
    "    \"a\", \"an\", \"and\", \"or\", \"this\", \"that\", \"be\", \"was\", \"were\", \"has\", \"had\", \n",
    "    \"have\", \"as\", \"but\", \"if\", \"then\", \"so\", \"because\", \"about\", \"into\", \"out\", \n",
    "    \"over\", \"under\", \"between\", \"after\", \"before\", \"above\", \"below\", \"again\", \n",
    "    \"once\", \"during\", \"while\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "])\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        text = text.strip()  # Remove extra spaces\n",
    "        text = \" \".join([word for word in text.split() if word not in stopwords_list])  # Remove stopwords\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "bank_transaction[\"cleaned_description\"] = bank_transaction[\"description\"].apply(clean_text)\n",
    "\n",
    "user_profile.columns = user_profile.columns.str.lower()  # Convert column names to lowercase\n",
    "bool_cols = user_profile.select_dtypes(include=['bool']).columns\n",
    "encoder = OneHotEncoder(drop='if_binary', dtype=int)\n",
    "encoded_array = encoder.fit_transform(user_profile[bool_cols]).toarray()\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=bool_cols)\n",
    "user_profile[bool_cols] = encoded_df\n",
    "\n",
    "combined = pd.merge(bank_transaction, user_profile, on=\"client_id\", how=\"left\")\n",
    "combined = combined.dropna(subset=[\"category\"])\n",
    "combined[\"category\"] = combined[\"category\"].astype(\"category\").cat.codes\n",
    "\n",
    "features = [\"cleaned_description\", \"txn_date\", \"amount\"] + list(bool_cols)\n",
    "X = combined[features]\n",
    "y = combined[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=100), \"cleaned_description\"),  # Reduce TF-IDF features\n",
    "    (\"amount\", StandardScaler(), [\"amount\"]),  # Normalize amount\n",
    "], remainder=\"drop\")\n",
    "\n",
    "model = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1))  # Reduce tree count\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy {accuracy}\")\n",
    "print(f\"Classfication report\\n{classification_rep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

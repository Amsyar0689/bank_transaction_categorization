{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb17cf5f",
   "metadata": {},
   "source": [
    "## Bank Transaction Categorization Model Report\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "Financial technology applications increasingly rely on automatic categorization of financial transactions to support user budgeting, detect fraud, and provide meaningful insights. This report presents a machine learning pipeline to categorize bank transactions based on textual descriptions, transaction metadata, and user profile attributes. The task combines structured and unstructured data sources, representing a classic multi-modal classification problem.\n",
    "\n",
    "We aim to predict the transaction category given the transaction description (text), amount (numeric), and user profile information (categorical/boolean). The solution must be accurate, interpretable, and scalable.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Problem Definition and Algorithm\n",
    "\n",
    "#### 2.1 Task Definition\n",
    "\n",
    "**Input:**\n",
    "\n",
    "* Transaction record consisting of:\n",
    "\n",
    "  * `description` (free text)\n",
    "  * `amount` (float)\n",
    "  * `txn_date` (timestamp)\n",
    "  * `client_id` and related profile fields (boolean/categorical)\n",
    "\n",
    "**Output:**\n",
    "\n",
    "* A predicted transaction category from a fixed set (e.g., Groceries, Bills, Entertainment).\n",
    "\n",
    "**Challenge:**\n",
    "\n",
    "* Integrating unstructured (text) and structured (numerical, categorical) features\n",
    "* Handling class imbalance\n",
    "* Ensuring interpretability for financial use cases\n",
    "\n",
    "#### 2.2 Algorithm Definition\n",
    "\n",
    "We use a pipeline-based supervised machine learning classifier. The selected model is **Random Forest Classifier** due to its robustness to noise, ability to handle non-linear feature interactions, and support for mixed data types.\n",
    "\n",
    "**Pseudocode Overview:**\n",
    "\n",
    "```python\n",
    "1. Clean and preprocess the 'description' column:\n",
    "   - Lowercase, remove punctuation and stopwords\n",
    "   - Vectorize using TF-IDF (unigrams + bigrams)\n",
    "\n",
    "2. Process structured fields:\n",
    "   - Normalize 'amount'\n",
    "   - Extract date features (day_of_week, is_weekend)\n",
    "   - One-hot encode user profile booleans\n",
    "\n",
    "3. Concatenate all features into a combined dataset\n",
    "\n",
    "4. Train a Random Forest Classifier (n=50 trees)\n",
    "   - Use 5-fold cross-validation\n",
    "\n",
    "5. Evaluate using accuracy, F1-score, confusion matrix\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Transaction: \"Netflix Payment\"\n",
    "* TF-IDF: Token \"netflix\" gets high score\n",
    "* Amount: 13.99 → typical subscription\n",
    "* Category → Predicted: \"Entertainment\"\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Data Analysis and Preprocessing\n",
    "\n",
    "#### 3.1 Dataset Overview\n",
    "\n",
    "* Two datasets:\n",
    "\n",
    "  * `bank_transaction.csv` with transaction logs\n",
    "  * `user_profile.csv` with user-level behavior flags\n",
    "\n",
    "#### 3.2 Preprocessing Steps\n",
    "\n",
    "**Text (Description):**\n",
    "\n",
    "* Cleaned using regex\n",
    "* Stopwords removed manually\n",
    "* Vectorized using `TfidfVectorizer` with `ngram_range=(1,2)` for richer context\n",
    "\n",
    "**Numerical:**\n",
    "\n",
    "* `amount` scaled using standardization\n",
    "* Missing values handled by dropping sparse rows\n",
    "\n",
    "**Date:**\n",
    "\n",
    "* `txn_date` used to extract `month`, `weekday`, and `is_weekend`\n",
    "\n",
    "**Categorical/Boolean:**\n",
    "\n",
    "* Boolean flags (e.g., `is_student`, `has_joint_account`) converted via OneHotEncoder\n",
    "\n",
    "#### 3.3 Feature Importance\n",
    "\n",
    "* Top TF-IDF features: \"uber\", \"netflix\", \"atm\"\n",
    "* Key structured features: `amount`, `is_student`, `day_of_week`\n",
    "\n",
    "> Feature importances were visualized using Random Forest's built-in `feature_importances_`.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Model Selection and Justification\n",
    "\n",
    "#### 4.1 Alternatives Considered\n",
    "\n",
    "| Model               | Pros                                    | Cons                                           |\n",
    "| ------------------- | --------------------------------------- | ---------------------------------------------- |\n",
    "| Logistic Regression | Simple, interpretable                   | Underfits sparse text, weak on non-linear data |\n",
    "| Random Forest       | Robust to noise, handles mixed features | Slower training                                |\n",
    "| XGBoost             | High accuracy, efficient                | Requires parameter tuning                      |\n",
    "\n",
    "#### 4.2 Why Random Forest?\n",
    "\n",
    "* Handles high-dimensional sparse vectors (TF-IDF)\n",
    "* No strict assumptions on feature distribution\n",
    "* Supports ranking of features for explainability\n",
    "* Strong baseline before moving to deep models\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Evaluation and Results\n",
    "\n",
    "#### 5.1 Metrics\n",
    "\n",
    "* **Accuracy:** \\~78%\n",
    "* **Macro F1-score:** Balanced across categories\n",
    "* **Confusion Matrix:** Showed confusion in overlapping terms (e.g., \"Grab\" vs. \"Uber\")\n",
    "\n",
    "#### 5.2 Cross-Validation\n",
    "\n",
    "* 5-fold stratified validation used\n",
    "* Performance was consistent across folds (low variance)\n",
    "\n",
    "#### 5.3 Class-Wise Insights\n",
    "\n",
    "* Strong performance: Entertainment, Food, Transportation\n",
    "* Weaker performance: Miscellaneous or ambiguous text\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Future Development Plans\n",
    "\n",
    "#### 6.1 Short-Term (1 Month)\n",
    "\n",
    "* Use `RandomizedSearchCV` for hyperparameter tuning\n",
    "* Add `merchant_name` field if available\n",
    "* Implement `SMOTE` for class balancing\n",
    "* Optimize TF-IDF memory footprint with sparse matrices\n",
    "\n",
    "#### 6.2 Long-Term (3 Months)\n",
    "\n",
    "* Replace TF-IDF with pretrained embeddings (e.g., FastText or BERT)\n",
    "* Integrate XGBoost or LightGBM with early stopping\n",
    "* Deploy model as a REST API for real-time inference\n",
    "* Implement user-specific profile modeling (e.g., personal merchant history)\n",
    "* Use SHAP for interpretability dashboards\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Conclusion\n",
    "\n",
    "This project built a pipeline for categorizing transactions using both unstructured and structured data. Random Forest was chosen for its strong performance and interpretability. Through TF-IDF and user profile integration, the model achieved \\~78% accuracy. Future improvements will focus on feature engineering, deep text embeddings, and real-time deployment.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
